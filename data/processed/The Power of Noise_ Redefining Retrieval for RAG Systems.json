{
  "text": "The Power of Noise: Redefining Retrieval for RAG Systems\n\nFlorin Cuconasu\u2217\n\ncuconasu@diag.uniroma1.it\nSapienza University of Rome\nRome, Italy\n\nGiovanni Trappolini\u2217\n\ntrappolini@diag.uniroma1.it\nSapienza University of Rome\nRome, Italy\n\nFederico Siciliano\nsiciliano@diag.uniroma1.it\nSapienza University of Rome\nRome, Italy\n\nSimone Filice\nfilice.simone@gmail.com\nTechnology Innovation Institute\nHaifa, Israel\n\nCesare Campagnano\ncampagnano@di.uniroma1.it\nSapienza University of Rome\nRome, Italy\n\nYoelle Maarek\nyoelle@yahoo.com\nTechnology Innovation Institute\nHaifa, Israel\n\nNicola Tonellotto\nnicola.tonellotto@unipi.it\nUniversity of Pisa\nPisa, Italy\n\nFabrizio Silvestri\nfsilvestri@diag.uniroma1.it\nSapienza University of Rome\nRome, Italy\n\nABSTRACT\n\nRetrieval-Augmented Generation (RAG) has recently emerged as\na method to extend beyond the pre-trained knowledge of Large\nLanguage Models by augmenting the original prompt with relevant\npassages or documents retrieved by an Information Retrieval (IR)\nsystem. RAG has become increasingly important for Generative\nAI solutions, especially in enterprise settings or in any domain in\nwhich knowledge is constantly refreshed and cannot be memorized\nin the LLM. We argue here that the retrieval component of RAG\nsystems, be it dense or sparse, deserves increased attention from\nthe research community, and accordingly, we conduct the first com-\nprehensive and systematic examination of the retrieval strategy\nof RAG systems. We focus, in particular, on the type of passages\nIR systems within a RAG solution should retrieve. Our analysis\nconsiders multiple factors, such as the relevance of the passages in-\ncluded in the prompt context, their position, and their number. One\ncounter-intuitive finding of this work is that the retriever\u2019s highest-\nscoring documents that are not directly relevant to the query (e.g.,\ndo not contain the answer) negatively impact the effectiveness of\nthe LLM. Even more surprising, we discovered that adding random\ndocuments in the prompt improves the LLM accuracy by up to\n35%. These results highlight the need to investigate the appropriate\nstrategies when integrating retrieval with LLMs, thereby laying the\ngroundwork for future research in this area.1\n\nCCS CONCEPTS\n\n\u2022 Information systems \u2192Novelty in information retrieval.\n\n1The code and data are available at github.com/florin-git/The-Power-of-Noise\n*These authors contributed equally to this work.\n\nThis work is licensed under a Creative Commons Attribution\nInternational 4.0 License.\n\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\n\u00a9 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657834\n\nKEYWORDS\n\nRAG, LLM, Information Retrieval\n\nACM Reference Format:\nFlorin Cuconasu\u2217, Giovanni Trappolini\u2217, Federico Siciliano, Simone Fil-\nice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio\nSilvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Sys-\ntems. In Proceedings of the 47th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR \u201924), July 14\u201318,\n2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https:\n//doi.org/10.1145/3626772.3657834\n\n1\nINTRODUCTION\n\nLarge Language Models (LLMs) [9] have demonstrated unprece-\ndented proficiency in various tasks, ranging from text generation\nand complex question answering [6], to information retrieval (IR)\ntasks [22, 57]. However, LLMs have limitations in the handling of\nlong contexts [52], a constraint that leads to an increased reliance\non their pre-trained knowledge. This limitation not only confines\ntheir ability to effectively manage extended discourse, such as in\nbooks or long conversations, but also increases the probability of\ngenerating hallucinations, instances for which the model produces\nfactually incorrect or nonsensical information [41]. To improve the\naccuracy of responses generated by LLMs, Retrieval-Augmented\nGeneration (RAG) has emerged as a promising solution [28]. RAG\nis primarily designed to improve factual accuracy by providing\nthe model access to auxiliary information, thereby augmenting the\noriginal prompt with information not necessarily memorized in\nthe LLM. A key benefit of this approach is that it helps ground the\nprompt with relevant information that might help the LLM gener-\nate more accurate answers at inference time. At their core, RAG\nsystems consist of two fundamental components: a retriever and a\ngenerator. The retriever is responsible for invoking an external IR\nsystem (dense and/or sparse) and feeding the selected results to a\ngenerator component.\nThis study focuses on the IR aspect of RAG, posing the following\nresearch question: \u201cWhat characteristics are desirable in a retriever\nto optimize prompt construction for RAG systems? Are current re-\ntrievers ideal?\". We focus on the three main types of documents\n\n719\n\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\nFlorin Cuconasu et al.\n\n(or passages2) that a retriever can return: relevant, distracting, and\nrandom. Relevant documents contain pertinent information that\neither directly answers or might inform the query. Distracting doc-\numents, while not directly answering the query, are semantically\nor contextually linked to the topic. For instance, if one asks for\nthe color of Napol\u00e9on\u2019s horse, a passage describing the color of\nJos\u00e9phine de Beauharnais\u2019 (Napol\u00e9on\u2019s first wife) horse, while not\ncontaining the right information, would be highly related. Random\ndocuments have no relation whatsoever to the query and can be\nseen as a kind of informational noise within the retrieval process.\nOne of the key goals of our study is to determine the role of each\ntype of document and the relative value they bring to the LLM\neffectiveness. In particular, we verify whether there is a need to\nrevisit some of the commonly accepted assumptions in IR systems\nwhen used in the context of LLMs. The main contributions of our\nwork are the following:\n\n(1) We conduct the first comprehensive study examining the\nimpact of the type of retrieved documents in RAG on the\nLLM effectiveness.\n(2) We propose retrieval RAG heuristics that leverage the unex-\npected results of this study.\n(3) We release all associated code and data to the community to\nencourage further research.\n\n2\nRELATED WORKS\n2.1\nGenerative Language Models\n\nThe inception of the modern LLM era can be traced back to the\nseminal paper titled \u201cAttention Is All You Need\" [52]. This work in-\ntroduced the transformer architecture, a framework that adopts an\nattention mechanism instead of recurrent layers, enabling the model\nto capture global dependencies within the data. The following year,\nBERT (Bidirectional Encoder Representations from Transformers)\n[22] offered a significant improvement over the state-of-the-art via\na novel bidirectional, unsupervised language representation. The\nevolution of transformer-based models continued with the devel-\nopment of the Generative Pre-trained Transformer (GPT) [37]. Its\nsuccessor, GPT-2 [38], expanded upon this foundation with a larger\nscale model and demonstrated improved performance across a vari-\nety of language tasks without task-specific training. The subsequent\niteration, GPT-3 [9], represented a further enhancement in model\nscale and capabilities, particularly in the realm of few-shot learning.\nFinally, recent times have seen a surge in the production of large,\npublicly available language models. Several actors have released\ntheir models, most notably, Llama [49, 50], Falcon [1], Mosaic MPT\n[47], and Phi [16, 29]. There are also versions of these models that\nhave been fine-tuned on specific languages [5, 10, 12, 17, 43]. The\nproliferation and quality of these models are expanding the range\nof tasks and the vision they address [48, 54, 56].\n\n2.2\nInformation Retrieval\n\nFoundational information retrieval methodologies, such as the Vec-\ntor Space Model and the TF-IDF scoring [42] introduced in the\n\n2We interchangeably use here the terms \u201cpassage\" or \u201cdocument\" to represent the\nindexing/retrieval unit of the IR system.\n\n1980s are the basis for quantifying textual similarity. These re-\ntrieval methods are characterized by their use of high-dimensional\nand sparse feature vectors and have been essential in developing\na full generation of IR systems. BM25 represents the most famous\ncurrent iteration [40]. A significant evolution in IR is the intro-\nduction of dense retrievers, which emerged from advancements\nin deep learning; they utilize low-dimensional dense vectors for\ntextual representation, and allow to capture semantic relationships.\nThis is in contrast to traditional IR methods (referred to as sparse\nin opposition to dense), which typically rely on lexical match and\nstruggle with semantic match [32]. In the last few years, dense\nmethods such as DPR [19] and others [15, 24] have demonstrated\nthat they can compete with sparse methods.\n\n2.3\nRetrieve and Generate\n\nRAG introduces a new approach in AI, combining the strengths of\nboth retrieval-based and generative models. The concept of RAG\nwas coined and popularized in [28], which introduced a model that\ncombines a dense passage retriever with a sequence-to-sequence\nmodel, demonstrating substantial improvements in knowledge-\nintensive tasks. Similar methods/variations have also been pro-\nposed concurrently or soon after, such as [2, 4, 8, 13, 21]; see [33]\nfor a survey on augmented language models. Researchers and prac-\ntitioners have recently started to explore these RAG systems\u2019 in-\nner workings. Notably, [44, 51] analyzed the impact of different\ntypes of documents on cascading IR/NLP systems. Other works\nhave tried to study how attentive transformers are to their input\n[23, 30, 31, 39, 46]. [7] studied the effect of the retriever\u2019s similarity\nmetric, which was found to be insufficient for reasoning. In [25, 55],\nauthors analyzed LLM\u2019s receptiveness to external evidence against\ninternal memory. In [60], they test the model\u2019s (in)ability to ground\nreferences.\nIn this paper, we want to provide the first comprehensive analysis\nof the implications of using a retriever module in a RAG system,\nstudying the impact of several key factors, like the type, number,\nand position of documents that should augment the prompt to the\nLLM.\n\n3\nRAG\n\nIn this paper, we explore the application of RAG in the context of\nQuestion Answering, arguably its most popular application.\n\n3.1\nOpen-Domain Question Answering\n\nOpen-domain Question Answering (OpenQA) refers to the task\nof developing systems capable of providing accurate and contex-\ntually relevant answers to a broad range of questions posed in\nnatural language without limitations to specific domains or prede-\nfined datasets. In general, we want to find an answer A to a query\n\ud835\udc5e. To do so, we draw information from a corpus of documents\nD = {\ud835\udc511,\ud835\udc512, . . . ,\ud835\udc51\ud835\udc5b}, which is usually assumed to be large in size.\nA prevalent approach for this task involves a two-step architecture,\ntypically comprising a retriever and a reasoner (typically a gen-\nerator). This methodology addresses the inherent complexities of\nOpenQA by dividing the process into distinct phases: first finding\nthe appropriate set of documents that can potentially address the\n\n720\n\nThe Power of Noise: Redefining Retrieval for RAG Systems\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\n\nquery and then synthesizing an answer, which can be consumed\nby the user of the QA system.\n\n3.2\nRetriever\n\nThe retriever plays a critical role in the OpenQA task. Its goal\nis to find a sufficiently small subset of documents D\ud835\udc5fto allow\nthe reasoner to answer the query correctly. Among the various\nretrieval methodologies, the use of a dense retriever has gained\nprominence due to its effectiveness in handling semantic matches.\nDense retrieval requires transforming textual data into vector rep-\nresentations, which is typically achieved with a neural network,\noften a transformer-based encoder, like BERT [22]. The dense re-\ntriever processes both the query \ud835\udc5eand potential source documents\nto generate corresponding embeddings \u00ae\ud835\udc5efor the query and \u00ae\ud835\udc51\ud835\udc56for\neach document \ud835\udc51\ud835\udc56\u2208D. The embedding process can be represented\nas:\n\u00ae\ud835\udc5e= \ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc5e(\ud835\udc5e); \u00ae\ud835\udc51\ud835\udc56= \ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc51(\ud835\udc51\ud835\udc56)\n\nwhere \ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc5eand \ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc51are neural network-based encoders,\npotentially sharing weights or architecture, designed to map the\ntextual data into a vector space. Once the embeddings are generated,\nthe retrieval process involves computing the similarity between\nthe query embedding and each document embedding. The most\ncommon approach is to use dot product [20], defined as: \ud835\udc60(\ud835\udc5e,\ud835\udc51\ud835\udc56) =\n\u00ae\ud835\udc5e\u00b7 \u00ae\ud835\udc51\ud835\udc56. This score quantifies the relevance of each document to\nthe query by measuring their similarity in the embedded vector\nspace, with higher scores indicating greater relevance. According\nto these scores, the top-ranked documents are selected for further\nprocessing in the generator component.\n\n3.3\nReasoner\n\nThe second step involves a generator component in charge of syn-\nthesizing an answer, typically implemented via an LLM. Generative\nlanguage models operate by predicting the probability distribution\nof the next token, given the previous tokens. For a given sequence\nof words \ud835\udc641,\ud835\udc642, . . . ,\ud835\udc64\ud835\udc5b, a generative language model aims to max-\nimize the likelihood of this sequence, expressed using the chain\nrule of probability:\n\n\ud835\udc43(\ud835\udc641,\ud835\udc642, . . . ,\ud835\udc64\ud835\udc5b) =\n\n\ud835\udc41\n\u00d6\n\n\ud835\udc56=1\n\ud835\udc43(\ud835\udc64\ud835\udc56|\ud835\udc641,\ud835\udc642, . . . ,\ud835\udc64\ud835\udc56\u22121)\n\nwhere \ud835\udc43(\ud835\udc64\ud835\udc56|\ud835\udc641,\ud835\udc642, . . . ,\ud835\udc64\ud835\udc56\u22121) is the conditional probability of the\nword \ud835\udc64\ud835\udc56given the preceding sequence of words \ud835\udc641,\ud835\udc642, . . . ,\ud835\udc64\ud835\udc56\u22121.\nIn RAG, the generative language model takes a query \ud835\udc5eand the\nretrieved documents D\ud835\udc5fas input and generates a response by se-\nquentially predicting the next token in the sequence. More formally,\n\n\ud835\udc43\ud835\udc5f\ud835\udc4e\ud835\udc54(\ud835\udc66|\ud835\udc5e) \u2248\n\n\ud835\udc41\n\u00d6\n\n\ud835\udc56\n\n\u2211\ufe01\n\n\ud835\udc51\u2208D\ud835\udc5f\n\ud835\udc5d\ud835\udf02(\ud835\udc51|\ud835\udc5e)\ud835\udc5d\ud835\udf03(\ud835\udc66\ud835\udc56|\ud835\udc5e,\ud835\udc51,\ud835\udc661:\ud835\udc56\u22121),\n\nwhere \ud835\udc5d\ud835\udf02(\ud835\udc51|\ud835\udc5e) is the retrieval component that provides a (trun-\ncated) probability distribution for the top-scoring documents, and\n\ud835\udc5d\ud835\udf03(\ud835\udc66\ud835\udc56|\ud835\udc5e,\ud835\udc51,\ud835\udc661:\ud835\udc56\u22121) is a probability distribution parameterized by \ud835\udf03\nthat generates a current token based on the previously generated\ntokens, the query, and the retrieved document; this role is filled by\nthe LLM. In the case of dense retrieval, the probability distribution\nfor the top-scoring documents may assume a functional form of\n\nthe kind \ud835\udc5d\ud835\udf02(\ud835\udc51|\ud835\udc5e) \u221dexp(\u00ae\ud835\udc5e\u00b7 \u00ae\ud835\udc51). Given our formalization of the RAG\ntask, we notice how the generative component \ud835\udc5d\ud835\udf03depends on a\ngiven text, that is the query, and a dynamic text, that is the set of\nretrieved documents. We study in the next two sections the impact\nof changing the set of retrieved documents on the generator and,\nconsequently, the whole end-to-end system. In particular, we aim\nto find the best set of documents D\ud835\udc5fthat a retriever should feed\nthe generator to maximize the system\u2019s effectiveness.\n\n4\nEXPERIMENTAL METHODOLOGY\n\nIn this section, we detail the experimental framework. We start by\ndescribing the data used in the experiments and then discuss the\ntype of documents that a retriever can return and pass to the LLM.\n\n4.1\nNatural Question Dataset\n\nThe Natural Questions (NQ) dataset [26] is a large-scale collection\nof real-world queries derived from Google search data. Each en-\ntry in the dataset consists of a user query and the corresponding\nWikipedia page containing the answer. The NQ-open dataset [27], a\nsubset of the NQ dataset, differs by removing the restriction of link-\ning answers to specific Wikipedia passages, thereby mimicking a\nmore general information retrieval scenario similar to web searches.\nThis open-domain nature significantly impacts our experimental de-\nsign, particularly in the selection and categorization of documents.\nFollowing the methodology of Lee et al. [27], our primary source\nfor answering queries is the English Wikipedia dump as of 20 De-\ncember 2018. Consistently with the Dense Passage Retrieval (DPR)\napproach [20], each Wikipedia article in this dump was segmented\ninto non-overlapping passages of 100 words. A significant challenge\nin open-domain question answering is the potential temporal mis-\nmatch between the Wikipedia dump and the question-answer pairs\nin the dataset, which can lead to missing answers in the dataset,\nas highlighted in the AmbigQA study [34]. To mitigate this, we\nintegrated the gold documents from the original NQ dataset into\nour Wikipedia document set. Given the open-domain nature of our\ntask, there may be additional documents relevant to the query, i.e.,\ncontaining the answer, but we will not consider them as gold. The\nfinal dataset comprises 21, 035, 236 documents, with 72, 209 queries\nin the train set and 2, 889 in the test set.\n\n4.2\nTypes of Documents\n\nIn our study, we categorize documents into four distinct types,\neach represented by a unique symbol, based on their relevance and\nrelationship to the queries:\n\n\u22c6Gold Document. The gold document, identified by \u22c6, refers\nto the original context in the NQ dataset, specifically the passage of\na Wikipedia page containing the answer and contextually relevant\nto a given query.\n\n\u00ae Relevant Documents. Denoted by \u00ae, relevant documents are\npassages that, akin to the gold document, contain the correct answer\nand are contextually useful for answering the query. They provide\nadditional sources of information that are correct and pertinent to\nthe query. Notably, the gold document is a relevant document.\n\na Distracting Documents. Symbolized by a, distracting docu-\nments are semantically similar to the query but do not contain the\n\n721\n\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\nFlorin Cuconasu et al.\n\ncorrect answer. They serve a crucial role in evaluating the genera-\ntor\u2019s proficiency in discerning between relevant and non-relevant\ninformation. In practice, these are the top-scoring retrieved docu-\nments that are not relevant.\n\nRandom Documents. Indicated by\n, random documents are\nneither related to the query nor contain the answer. They are in-\nstrumental in assessing the model\u2019s ability to handle completely\nunrelated information. In practice, in our tests, we will randomly\nsample these documents from the corpus.\n\nIn our analysis, the entire set of documents fetched by the retriever\nis represented by the symbol q. This possibly encompasses all doc-\nument types \u2014 gold, relevant, distracting, or random \u2014 and serves\nto discuss the retrieval output in a generalized manner without\nspecifying individual document categories.\n\n4.3\nDocument Retrieval\n\nOur methodology utilizes a two-step approach in line with a typical\nRAG setting, as explained in Section 3.2. As the first component,\nour experiments use Contriever [15], a BERT-based dense retriever,\nas the default retriever. It is trained without supervision using a\ncontrastive loss. To enhance the efficiency of similarity searches\nwithin our corpus, comprising about 21 million documents, we also\nemploy the FAISS IndexFlatIP indexing system [11]. The embedding\nof each document and query is obtained by averaging the hidden\nstate of the last layer of the model.\n\n4.4\nLLM Input\n\nUpon receiving a query, the retriever selects the top-\ud835\udc58documents\nfrom the corpus according to a given similarity measure. These\ndocuments, in conjunction with the task instruction and the query,\nconstitute the input for the LLM to generate a response. The NQ-\nopen dataset was structured to include only those queries whose\nanswers consist of no more than five tokens [27]. Consequently,\nthe LLM is tasked with extracting a query response, confined to a\nmaximum of five tokens, from the provided documents. The input is\nencoded into a prompt, whose template is shown in Figure 1, begin-\nning with the task instruction, presented in italics for clarity. This\nis followed by the context, which comprises the selected documents\nfollowed by the query string. This prompt design aligns with the\nmethodological approach outlined in [30]. While the composition\nof the context will vary according to the single experiment, the\ninstruction will always be placed at the beginning of the prompt\nand the query always at the end.\n\n4.5\nLLMs Tested\n\nWe consider several LLMs in our experiments. Consistently across\nall models, we adopt a greedy generation approach with a maxi-\nmum response length of 15 tokens. Acknowledging the constraints\nimposed by memory and computational resources, we have im-\nplemented a model quantization strategy, reducing all models to a\n4-bit representation. Besides the above prompt, the models are not\nprovided with additional exemplars for few-shot learning, which,\nwhile of interest, is outside the scope of this paper. We conduct tests\non both the base and the instruct versions of the LLMs. However, we\n\nLLM Input - Only Gold \u22c6\n\nYou are given a question and you MUST respond by EX-\nTRACTING the answer (max 5 tokens) from one of the pro-\nvided documents. If none of the documents contain the answer,\nrespond with NO-RES.\nDocuments:\nDocument [3](Title: Millennium Falcon) Han Solo won\nthe Millennium Falcon from Lando Calrissian in the card\ngame sabacc...\n\nQuestion: who owned the millennium falcon be-\nfore han solo\nAnswer: Han Solo\n\nFigure 1: Example LLM input with an erroneous output, high-\nlighted in red. The input consists of an italicized task instruc-\ntion, followed by the context (documents), and the query. The\nLLM\u2019s response is marked under \u2018Answer\u2019. The gold color\nhighlights both the gold document and the correct answer,\n\u201cLando Calrissian\u201d, indicating the expected source and con-\n\ntent of the accurate response.\n\nonly report on the latter, as while the behavior is consistent across\nboth, the instruct versions demonstrate superior performance.\n\n\u2022 Llama2. The 7B parameters version of the Llama2 family [50]\n\nshows state-of-the-art performance on most downstream\ntasks compared to models of the same size. It was trained\nwith a 4096 tokens context window and uses multi-query\nattention [45].\n\u2022 Falcon. Falcon 7B, the smallest model of the Falcon series, [1]\n\nwas trained on the RefinedWeb dataset [35], a large, filtered,\nand deduplicated corpus. Similarly to Llama2, it uses multi-\nquery attention, with a context length of 2048 tokens.\n\u2022 Phi-2. This is the smallest model used in this work (2.7B\n\nparameters). Despite its modest size, it achieves performance\ncomparable to the other models [16, 29], thanks to its pre-\ntraining on \u201ctextbook-quality\u201d data. It has a context window\nof 2048 tokens.\n\u2022 MPT. This 7B parameters model uses ALiBi attention [36, 47]\n\nfor a virtually unlimited context length. In our experiments,\nto leverage the model\u2019s full potential, we set the limit to 2048\ntokens, i.e., the same used for the model\u2019s pre-training.\n\n4.6\nAccuracy\n\nThe NQ-open dataset allows a range of potential answers for each\nquery. Frequently, these answers are different variants of the same\nconcept (e.g., \u201cPresident D. Roosevelt\u201d or \u201cPresident Roosevelt\u201d),\nwhile in some cases, a single query may accept multiple distinct\ncorrect answers. To evaluate the accuracy of responses generated\nby LLMs, we use an assessment technique in line with [18, 30].\nThis methodology examines whether at least one of the predefined\ncorrect answers is contained within the response produced by the\nLLM. We measure the correctness of the LLM\u2019s responses as either\n\n722\n\nThe Power of Noise: Redefining Retrieval for RAG Systems\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\n\naccurate or inaccurate based on the presence of the answer in a\nbinary fashion. Nevertheless, this evaluation strategy is not without\nchallenges. A principal issue arises in determining response cor-\nrectness, particularly in instances involving date representations\nor varying phrasings conveying identical meanings. For example,\nif the LLM generates \u201cRoosevelt\u201d in response to a query where the\nestablished correct answer is \u201cPresident Roosevelt\u201d, the response\nwould be deemed incorrect under our current evaluation schema.\nRecognizing this limitation, we acknowledge the necessity for a\nmore advanced analysis of answer variations, which we leave to\nfuture research.\n\n5\nRESULTS\n\nStudying the characteristics of optimal prompts for RAG systems\ncorresponds to answering our research question (RQ): \"What char-\nacteristics are desirable in a retriever to optimize prompt construction\nfor RAG systems in order to increase the LLM effectiveness?\". More\nspecifically, we focus on three essential elements of the configura-\ntion: type, number, and positioning of the documents, and for each,\nwe test various prompt combinations. To facilitate the understand-\ning of our experimental setup, we employ a streamlined schema for\nrepresenting the composition of prompts via the following symbols:\n[I, \u22c6, \u00ae, a,\n, Q]. The task instruction (I) and the query (Q) are\nconsistently positioned at the beginning and end, respectively. The\nmiddle section varies and represents different contextual elements\n- in this instance, these are gold, relevant, distracting, and random,\nappearing in that specific sequence. Additionally, the number of\ncontextual documents is a variable in its own right and will be\nreported in the results tables below.\n\n5.1\nImpact of Distracting Documents\n\nLLM Input - Distracting a and Gold \u22c6\n\nTask Instruction...\nDocuments:\nDocument [1](Title: Han Solo) Before the events of the\nfilm, he and Chewbacca had lost the \u201cMillennium Falcon\u201d\nto thieves, but they reclaim the ship after it...\nDocument [2](Title: Millennium Falcon) The \u201cFalcon\u201d has\nbeen depicted many times in the franchise, and ownership\nhas changed several times...\nDocument [3](Title: Millennium Falcon) Han Solo won\nthe Millennium Falcon from Lando Calrissian in the card\ngame sabacc...\n\nQuestion: who owned the millennium falcon be-\nfore han solo\nAnswer: Han Solo\n\nFigure 2: Example LLM input with an erroneous output, high-\nlighted in red. The context of the prompt is composed of\ndistracting documents and the gold near the query. The task\ninstruction is as in Figure 1.\n\nIn our first set of experiments, we use a selection of 10K queries\nfrom the training set of the NQ-open dataset and assume an oracle\nsetup in which the gold document for the query is known. To this\neffect, we add to the gold document a set of distracting documents,\ni.e., documents with high retrieval scores but not containing the\nanswer, in order to measure their impact on the system; schemat-\nically [I, a, \u22c6, Q]. Figure 2 shows an example of this setup\u2019s\nvisualization. Results of this experiment are shown in Table 1 (far,\nmid, and near relate to the distance between the gold document\nand the query; more details in the following sub-section). A crit-\nical observation emerging from this analysis is a clear pattern of\nprogressive accuracy degradation as the number of distracting doc-\numents included in the context increases. This was observed across\nall LLMs, with accuracy deteriorating by more than 0.38 (\u221267%)\nin some cases. Even more importantly, adding just one distracting\ndocument causes a sharp reduction in accuracy, with peaks of 0.24\n(\u221225%), as can be seen by comparing the row with 0 distracting\ndocuments (only gold scenario, as seen in Figure 1) with that of 1\ndistracting document. This experiment highlights a critical issue\nfor RAG systems, particularly in real-world IR settings where re-\nlated but non-answer-containing documents are commonplace. Our\nempirical analysis suggests that introducing semantically aligned\nyet non-relevant documents adds a layer of complexity, potentially\nmisguiding LLMs away from the correct response. A visual ex-\nplanation can be seen in Figure 3, which illustrates the attention\nscores within the prompt\u2019s context for a specific example in which\nthe LLM incorrectly answers. This figure highlights the model\u2019s\ndisproportionate focus on a distracting document (leftmost) at the\nexpense of the gold document (rightmost), likely contributing to\nthe erroneous response. Note that for consistency of results across\nLLMs, we need to account for their various input token capabilities:\nLlama2 can process up to 4096 tokens, but other models are lim-\nited to 2048 tokens. This led to the exclusion of evaluations with a\nhigher number of distracting documents (namely greater than 10)\nas reflected by the empty values in the tables.\n\nIn addition, we wanted to verify that our results were not overly\ndependent on the type of dense retrieval system we used. We\nwanted, in particular, to check whether another dense retriever\nspecifically trained on \u201chard negatives\" would better distinguish\nbetween directly relevant and distracting documents, potentially\nleading to different results. To explore this hypothesis, we used\nADORE [59], a state-of-the-art retriever trained with \u201cdynamic\nhard negatives\u201d, to select the distracting documents. In scenarios\nwith 1, 2, and 4 distracting documents in the [I, a, \u22c6, Q] setting\nwith Llama2, we obtain an accuracy of 0.4068, 0.3815, and 0.3626,\nrespectively. This is significantly lower than the baseline accuracy\nof 0.5642, where no distracting documents were included, and than\nthe results obtained with Contriever in the same settings. We con-\nclude from this that distinguishing between relevant and distracting\ninformation is a hard problem that cannot be mitigated simply by\nchanging the dense retrieval method at this stage.\n\n5.2\nImpact of Gold Positioning\n\nWe conduct here another experiment where we systematically shift\nthe position of the gold document within the context to study its\n\n723\n\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\nFlorin Cuconasu et al.\n\nTable 1: Accuracy results of the LLMs when evaluated with prompts composed of the gold document \u22c6and a varying number\nof distracting a documents. The table illustrates how the inclusion of an increasing number of distracting documents affects\nLLM\u2019s performance. Scenarios where the prompt exceeded the model\u2019s input limit, leading to potential data truncation, are not\nincluded ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document\nscenario [I, \u22c6, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores\nfor the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).\n\nFar - [I, \u22c6, a, Q]\nMid - [I, a, \u22c6, a, Q]\nNear - [I, a, \u22c6, Q]\n\n# a\nLlama2\nMPT\nPhi-2\nFalcon\nLlama2\nMPT\nPhi-2\nFalcon\nLlama2\nMPT\nPhi-2\nFalcon\n\n0\n0.5642\n0.2148\n0.4438\n0.4330\n0.5642\n0.2148\n0.4438\n0.4330\n0.5642\n0.2148\n0.4438\n0.4330\n1\n0.4586\n0.1976\n0.3585\n0.3469\nno-mid\nno-mid\nno-mid\nno-mid\n0.4283\n0.1791\n0.4227\n0.3602\n2\n0.3455\n0.1913\n0.3430\n0.3246\n0.3322\n0.1802\n0.3375\n0.2823\n0.3974\n0.2002\n0.3975\n0.3111\n4\n0.2745\n0.2209*\n0.3019\n0.2670\n0.2857\n0.1775\n0.2885\n0.2378\n0.3795\n0.2059*\n0.3701\n0.2736\n6\n0.2898\n0.2171*\n0.2943\n0.2392\n0.2698\n0.1424\n0.2625\n0.2103\n0.3880\n0.1892\n0.3623\n0.2656\n8\n0.2643\n0.2077*\n0.2513\n0.1878\n0.2268\n0.1002\n0.2360\n0.1745\n0.3748\n0.1944\n0.3423\n0.2424\n10\n0.2537\n-\n-\n-\n0.2180\n-\n-\n-\n0.3716\n-\n-\n-\n12\n0.2688\n-\n-\n-\n0.2382\n-\n-\n-\n0.3991\n-\n-\n-\n14\n0.2583\n-\n-\n-\n0.2280\n-\n-\n-\n0.4118\n-\n-\n-\n16\n0.2413\n-\n-\n-\n0.2024\n-\n-\n-\n0.3889\n-\n-\n-\n18\n0.2348\n-\n-\n-\n0.1795\n-\n-\n-\n0.3781\n-\n-\n-\n\nTable 2: Accuracy results of the LLMs when evaluated with prompts composed of the gold document \u22c6and a varying number\nof random\ndocuments. Surprisingly, increasing the number of random documents in the Near setting improves LLM\u2019s\nperformance. Scenarios where the prompt exceeded the model\u2019s input limit, leading to potential data truncation, are not\nincluded ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document\nscenario [I, \u22c6, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores\nfor the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).\n\nFar - [I, \u22c6,\n, Q]\nMid - [I,\n, \u22c6,\n, Q]\nNear - [I,\n, \u22c6, Q]\n\n#\nLlama2\nMPT\nPhi-2\nFalcon\nLlama2\nMPT\nPhi-2\nFalcon\nLlama2\nMPT\nPhi-2\nFalcon\n\n0\n0.5642\n0.2148\n0.4438\n0.4330\n0.5642\n0.2148\n0.4438\n0.4330\n0.5642\n0.2148\n0.4438\n0.4330\n1\n0.4733\n0.2447\n0.4329\n0.4035\nno-mid\nno-mid\nno-mid\nno-mid\n0.4862\n0.2125*\n0.4587\n0.4091\n2\n0.3776\n0.2639\n0.4249\n0.3805\n0.3928\n0.2584\n0.4293\n0.3612\n0.5032\n0.2660\n0.4614\n0.3912\n4\n0.3109\n0.2933\n0.4091\n0.3468\n0.3998\n0.2577\n0.3985\n0.3462\n0.5221\n0.2930\n0.4311\n0.3949\n6\n0.3547\n0.3036\n0.4130\n0.3250\n0.4138\n0.2265\n0.3891\n0.3196\n0.5681*\n0.2890\n0.4388\n0.3908\n8\n0.3106\n0.3039\n0.3812\n0.2543\n0.3734\n0.1566\n0.3596\n0.2767\n0.5609*\n0.2911\n0.4258\n0.3704\n10\n0.3390\n-\n-\n-\n0.3675\n-\n-\n-\n0.5579*\n-\n-\n-\n12\n0.3736\n-\n-\n-\n0.3641\n-\n-\n-\n0.5836\n-\n-\n-\n14\n0.3527\n-\n-\n-\n0.3372\n-\n-\n-\n0.5859\n-\n-\n-\n16\n0.3401\n-\n-\n-\n0.3159\n-\n-\n-\n0.5722\n-\n-\n-\n18\n0.3466\n-\n-\n-\n0.2982\n-\n-\n-\n0.5588*\n-\n-\n-\n\nimpact on the model\u2019s effectiveness. We define the positions of the\ngold document as follows:\n\n\u2022 Near: placed adjacent to the query in the prompt [I, a, \u22c6,\nQ] (as in Figure 2)\n\u2022 Mid: inserted in the middle of the context [I, a, \u22c6, a, Q]\n\u2022 Far: positioned as far as possible from the query in the con-\ntext [I, \u22c6, a, Q]\n\nResults in these settings partially corroborate evidence from [30].\nThe accuracy is higher when the gold document is near the query,\nlower when the gold document is furthest from it, and lowest when\n\nthe gold document is placed in the middle of the context. For in-\nstance, Llama2, with 18 distracting documents, reaches an accuracy\nof 0.37, 0.23, and 0.17, respectively. These results are consistent\nacross all models tested in the setting with distracting documents.\n\n5.3\nImpact of Noise\n\nWe devise an additional experimental setting aimed at evaluating\nthe robustness of the RAG system against noise. To this effect, we\ntake the gold document and add to it a certain number of docu-\nments picked at random from the corpus; see an example in Figure\n4. Against our expectations, the performance does not deteriorate\n\n724\n\nThe Power of Noise: Redefining Retrieval for RAG Systems\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\n\nTable 3: Accuracy of Llama2-7b in configurations involving random Wikipedia documents and retrieved documents [I,\n, q, Q].\nRows denote the number of random documents\nadded, and columns show the quantity of retrieved documents q. The left\nsection reports results using Contriever, and the right section using BM25. Scenarios where the prompt exceeded the model\u2019s\ninput limit, leading to potential data truncation, are not included ( - ). Each value not marked with an asterisk * represents a\nstatistically significant change from the base case of retrieved documents only [I, q, Q] (first row), as determined by a Wilcoxon\ntest (p-value < 0.01).\n\nContriever\nBM25\n\n#\n\n# q\n1\n2\n3\n4\n5\n8\n10\n1\n2\n3\n4\n5\n8\n10\n\n0\n0.1620\n0.1866\n0.1876\n0.1866\n0.1921\n0.2198\n0.2108\n0.2008\n0.2208\n0.2084\n0.2028\n0.2243\n0.2492\n0.2447\n1\n0.1308\n0.1616\n0.1717\n0.1893*\n0.1987*\n0.2153*\n0.2146*\n0.1568\n0.1963\n0.1921\n0.2115\n0.2295*\n0.2475*\n0.2506*\n2\n0.1315\n0.1644\n0.1859*\n0.2008\n0.2174\n0.2156*\n0.2368\n0.1644\n0.1973\n0.2080*\n0.2281\n0.2558\n0.2495*\n0.2596\n3\n0.1301\n0.1727\n0.2008\n0.2316\n0.2201\n0.2198\n0.2409\n0.1568\n0.2063\n0.2160\n0.2520\n0.2579\n0.2644\n0.2707\n5\n0.1464\n0.2056\n0.2233\n0.2240\n0.2150\n0.2451\n0.2482\n0.1772\n0.2402\n0.2437\n0.2520\n0.2554\n0.2804\n0.2866\n8\n0.1734\n0.2066\n0.2336\n0.2375\n0.2454\n0.2416\n0.2364\n0.1994\n0.2451\n0.2579\n0.2769\n0.2817\n0.2859\n0.2777\n10\n0.1796\n0.2174\n0.2450\n0.2502\n0.2499\n0.2420\n-\n0.2108\n0.2589\n0.2734\n0.2835\n0.2935\n0.2853\n-\n15\n0.2018\n0.2354\n0.2551\n0.2530\n-\n-\n-\n0.2243\n0.2686\n0.2790\n0.2928\n-\n-\n-\n16\n0.2032\n0.2471\n0.2558\n-\n-\n-\n-\n0.2323\n0.2662\n0.2838\n-\n-\n-\n-\n17\n0.2039\n0.2426\n-\n-\n-\n-\n-\n0.2326\n0.2693\n-\n-\n-\n-\n-\n18\n0.2073\n-\n-\n-\n-\n-\n-\n0.2309\n-\n-\n-\n-\n-\n-\n\nFigure 3: This heatmap depicts the attention distribution\nacross the context documents from the example shown in\nFigure 2, relative to the answer generated by Llama2-7b in\na prompt structured as [I, a, \u22c6, Q]. Cell (i, j) denotes the\nmean attention that tokens in the generated answer allocate\nto the tokens of the i-th document within the j-th attention\nlayer. This mean attention for each document is calculated\nby averaging the attention scores across all its constituent\ntokens.\n\nin the presence of noise, as can be seen in Table 2. Instead, we ob-\nserve an improvement in performance under the best-performing\nsetting (near [I,\n, \u22c6, Q]), with an improvement of 0.08 (+36%) in\n\nLLM Input - Random\nand Gold \u22c6\n\nTask instruction...\nDocuments:\nDocument [140](Title: Richard Yates (novelist)) For much\nof his life, Yates\u2019s work met almost universal critical ac-\nclaim, yet not one of his books sold over 12,000 copies in...\nDocument [242](Title: Android version history) Code\nname Version number Initial release date API level Security\npatches (No codename ) 1.0 September 23...\nDocument [3](Title: Millennium Falcon) Han Solo won\nthe Millennium Falcon from Lando Calrissian in the card\ngame sabacc...\n\nQuestion: who owned the millennium falcon be-\nfore han solo\nAnswer: Lando Calrissian\n\nFigure 4: Example LLM input with a correct output, high-\nlighted in green. The context of the prompt is composed of\nrandom documents and the gold near the query. The task\ninstruction is as in Figure 1.\n\nthe case of MPT. Furthermore, we observe that different models\nexhibit distinct behaviors. Both Llama2 and Phi-2 showed improve-\nments in this setting when the noise is introduced furthest from\nthe query. However, when the noise is positioned in the far [I, \u22c6,\n\n, Q] and mid [I,\n, \u22c6,\n, Q] settings, these models exhibit a\ndecline in performance. Notably, this performance degradation is\nmuch less accentuated when compared to the earlier setting with\ndistracting documents. This suggests that while Llama2 and Phi-2\ncan effectively handle noise far from the query, their ability to sift\n\n725\n\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\nFlorin Cuconasu et al.\n\nthrough irrelevant information diminishes as the noise is placed\ncloser to it. The MPT model presented a unique response; it showed\nan improvement in performance under all settings. Standing out\nfrom the rest, the Falcon model did not exhibit an improvement in\nperformance as observed in other models with the introduction of\nnoise. Peculiarly enough, Falcon and Llama2 do not consistently ex-\nhibit a \u201clost in the middle\u201d phenomenon, having in some instances\nbetter accuracy in the mid than far setting, for instance, in the case\nwith 8 noisy documents added.\n\n5.4\nRAG in Practice\n\nTo address our primary Research Question (RQ) about the char-\nacteristics of an effective RAG retriever, and following the results\nreported above, we now consider a more realistic scenario than an\noracle setup. Namely, given a query, we retrieve a set of documents\nthat can be either relevant or distracting. We then add random doc-\numents to this set of retrieved ones, schematically: [I,\n, q, Q]. For\nthis second set of experiments, we use the test set of the NQ-open\ndataset. Results for this experiment, using Llama2, can be seen on\nthe left side of Table 3. These results show that, regardless of the\nnumber of retrieved documents, adding random documents up until\nthe context length is filled is almost always beneficial, with gains\nin terms of accuracy up to 0.07 (+35%) in the case of 4 retrieved\ndocuments.\n\n5.4.1\nTesting Sparse Retrievers. In an effort to validate our initial\nobservations, we replicate our experiment using a sparse retrieval\napproach, specifically BM25. The corresponding results are outlined\nin the right section of Table 3. Consistent with earlier findings, we\nobserve that including random documents leads to an improvement\nin the effectiveness of the LLM. Notably, the use of BM25 yields\nan average increase in accuracy of 3-4 percentage points. This im-\nprovement is attributed to the quality of documents retrieved by\nBM25. We quantitatively evaluate the effectiveness of the retrieval\nmethods by computing the top-\ud835\udc58accuracy for varying numbers of\nretrieved documents. Note that this heuristic, while indicative, does\nnot capture the full spectrum of relevance. Our evaluation, based on\nthe presence of correct answers within documents, might overlook\nthe context-specific relevance due to potential lexical matches of\nthe answer string in documents. Despite this limitation, this method\naligns with established computational practices in literature [15, 19].\nIn our analysis, BM25 demonstrated higher relative top-\ud835\udc58accuracy\n(0.2966, 0.4105, 0.5237, 0.6663 for \ud835\udc58= 1, 2, 4, 10) compared to those\nof Contriever (0.2502, 0.3569, 0.4784, 0.6085 for the same \ud835\udc58), under-\nscoring its effectiveness in retrieving more relevant documents in\nour experimental setup.\n\n5.4.2\nIncreasing The Randomness. While our previous experiments\nshow the benefits of adding random documents, one might argue\nthat these documents are not totally random as they originate from\nthe same corpus (Wikipedia) and that they might help the LLM\nanswer in a fashion that is consistent with the corpus. For this rea-\nson, we carry out another experiment in which random documents\nare drawn from a drastically different corpus in terms of tone and\nstyle, namely Reddit Webis-TLDR-17 dataset [53]. The results are\noutlined on the left of Table 4. The inclusion of documents from the\nReddit corpus not only maintains the observed increase in accuracy\n\nbut even enhances it, with an improvement of 0.023 (+9% accuracy)\nwhen compared to the previous best score. Pushing the randomness\neven further, we carry out another test where we consider nonsen-\nsical sentences made up of random words as random documents.\nRemarkably, even in this scenario, we observe a performance im-\nprovement when compared to the base case of Wikipedia random\ndocuments, as shown in the right side of Table 4.\n\n5.4.3\nFalcon. As shown in Table 2, Falcon does not reach the same\nperformance increase when random documents are added to the\ngold document [I,\n, \u22c6, Q]. Accordingly, we want to verify whether\nit behaves differently when adding retrieved rather than gold docu-\nments. We find that the addition of random documents on top of\nretrieved documents [I,\n, q, Q] does improve the effectiveness of\nFalcon; see detailed results in Table 5. These results are in contrast\nwith the ones obtained in the oracle setting, where Falcon was\nrobust to noise. This new finding further validates our experimen-\ntal evidence, namely that, outside the oracle setting, all the tested\nmodels show an improvement when a certain amount of noise is\nadded.\n\n5.5\nRetriever Trade-Off\n\nThe experimental evidence detailed above not only contradicts the\ncommon perception that semantically close documents are help-\nful for LLMs but also highlights the need for a delicate balance\nbetween relevant and random documents. When arranged as de-\nscribed, random documents seem to exert a positive influence on\nLLM accuracy. However, for the LLM to generate accurate answers,\nsome degree of relevant information must exist in the context. On\nthe other hand, an overabundance of retrieved documents increases\nthe likelihood of including distracting and non-relevant informa-\ntion, leading to a sharp decline in performance. While establishing\na formal or comprehensive theory behind these findings remains an\nopen research challenge, we can still infer that there seems to be a\ntrade-off between the number of relevant and totally irrelevant doc-\numents. More specifically, we observed that the best effectiveness is\nachieved when a minimal set of documents is initially retrieved and\nthen supplemented with random documents until the context limit\nis reached. For the queries examined in this study, retrieving be-\ntween 3 and 5 documents is the most effective choice. Adding more\nincreases the risk of including too many distracting, thus coun-\nterproductive, documents. We argue here that there is a pressing\nneed for further research towards investigating how these initial\nfindings can be exploited. More importantly, it is evident that we\nhave yet to refine our understanding of the retriever\u2019s role within\na RAG system.\n\nOn The Unreasonable Effectiveness Of Random Documents. We can-\nnot close this paper without attempting to explain the results shown\nup to this point. We refer back to our RAG formulation, particularly\nthe conditioned function \ud835\udc5d\ud835\udf03(\ud835\udc66|\u00b7,\ud835\udc51). In hindsight, we can now state\nthat by adding random documents to the context, we are better\nconditioning this function, inducing enhanced accuracy. Previous\nresearch [3, 14], particularly [58], hints that there might be cases\nin which a pathologically low attention entropy causes the LLM to\ngenerate degenerate outputs with a sharp decrease in performance.\nThese episodes are named entropy collapse. Following this line of\n\n726\n\nThe Power of Noise: Redefining Retrieval for RAG Systems\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\n\nTable 4: Accuracy of Llama2-7b in configurations involving random documents and retrieved documents by Contriever [I,\n,\nq, Q]. Rows denote the number of random documents\nadded, and columns show the quantity of retrieved documents q.\nThe left section reports results with random documents from Reddit and the right section with nonsensical sentences made\nup of random words. Scenarios where the prompt exceeded the model\u2019s input limit, leading to potential data truncation, are\nnot included ( - ). Each value not marked with an asterisk * represents a statistically significant change from the base case of\nretrieved documents only [I, q, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01).\n\nRandom from Reddit\nRandom Words\n\n#\n\n# q\n1\n2\n3\n4\n5\n8\n10\n1\n2\n3\n4\n5\n8\n10\n\n0\n0.1620\n0.1866\n0.1876\n0.1866\n0.1921\n0.2198\n0.2108\n0.1620\n0.1866\n0.1876\n0.1866\n0.1921\n0.2198\n0.2108\n1\n0.1693*\n0.1931\n0.1845*\n0.1907\n0.2008\n0.2084\n0.2084\n0.1744\n0.1924*\n0.1969\n0.2077\n0.2091\n0.2139*\n0.2073*\n2\n0.1886\n0.2018\n0.2101\n0.2143\n0.2160\n0.2222*\n0.2219\n0.1765\n0.1855*\n0.2094\n0.2122\n0.2181\n0.2045\n0.2084*\n3\n0.1897\n0.2108\n0.2212\n0.2340\n0.2371\n0.2326\n0.2319\n0.1755\n0.1990\n0.2166\n0.2201\n0.2288\n0.2032\n0.2156*\n5\n0.1897\n0.2215\n0.2388\n0.2468\n0.2409\n0.2769\n0.2451\n0.1862\n0.2139\n0.2319\n0.2367\n0.2232\n0.2184*\n0.2278\n8\n0.2011\n0.2326\n0.2354\n0.2489\n0.2440\n0.2568\n0.2364\n0.1973\n0.2274\n0.2319\n0.2316\n0.2305\n0.2357\n0.2412\n10\n0.2053\n0.2326\n0.2451\n0.2534\n0.2551\n0.2658\n-\n0.2053\n0.2271\n0.2340\n0.2385\n0.2406\n0.2499\n-\n15\n0.2240\n0.2489\n0.2689\n0.2786\n-\n-\n-\n0.2215\n0.2416\n0.2589\n0.2634\n-\n-\n-\n16\n0.2240\n0.2561\n0.2676\n-\n-\n-\n-\n0.2219\n0.2437\n0.2568\n-\n-\n-\n-\n17\n0.2243\n0.2565\n-\n-\n-\n-\n-\n0.2201\n0.2450\n-\n-\n-\n-\n-\n18\n0.2240\n-\n-\n-\n-\n-\n-\n0.2177\n-\n-\n-\n-\n-\n-\n\nTable 5: Accuracy of Falcon-7b on Reddit data in the random\n+ retrieved setting [I,\n, q, Q]. Rows denote the number of\nrandom documents\nadded, and columns show the quan-\ntity of retrieved documents q. Scenarios where the prompt\nexceeded the model\u2019s input limit, leading to potential data\ntruncation, are not included ( - ). Each value not marked with\nan asterisk * represents a statistically significant change from\nthe base case (first row), as determined by a Wilcoxon test\n(p-value < 0.05).\n\n#\n\n# q\n1\n2\n3\n4\n5\n9\n\n0\n0.1568\n0.1717\n0.1855\n0.1938\n0.1942\n0.1998\n1\n0.1551*\n0.1793*\n0.1897*\n0.1924*\n0.1976*\n-\n2\n0.1529*\n0.1762*\n0.1938*\n0.2011*\n0.1976*\n-\n3\n0.1599*\n0.1727*\n0.1911*\n0.2021*\n0.2118\n-\n4\n0.1606*\n0.1758*\n0.1959\n0.2073\n0.2108\n-\n5\n0.1627*\n0.1762*\n0.2000\n0.2108\n-\n-\n6\n0.1651*\n0.1848\n0.2004\n-\n-\n-\n7\n0.1675\n0.1848\n-\n-\n-\n-\n8\n0.1682\n-\n-\n-\n-\n-\n\nresearch, we measure the entropy of the attention scores in the\ncase where only the gold document is supplied [I, \u22c6, Q] against\nthe case in which random documents are added [I,\n, \u22c6, Q]. We\nfind that when we introduce random documents, the entropy of\nthe systems has a 3X increase. Although these experiments show a\npattern, we cannot yet answer this question in a definitive manner.\nWhile out of the scope of this work, which focuses on the retriever\ncomponent of RAG systems, we believe it is highly important to\ninvestigate the reasons for which the LLM shows this behavior.\nFuture studies should aim to elucidate why this noisy state is more\nadvantageous and identify the characteristics that contribute to its\neffectiveness.\n\n6\nCONCLUSIONS\n\nIn this paper, we conducted the first comprehensive study focus-\ning on the impact of retrieved documents on the RAG framework,\naiming to understand the traits required in a retriever to optimize\nprompt construction for a RAG system. This study led to several im-\nportant findings, including two unexpected ones. First, the position\nof relevant information should be placed near the query; otherwise,\nthe model seriously struggles to attend to it. Second, in contrast to\ncommon perception, top-scoring retrieved documents that do not\ncontain the answer, when added to a prompt, negatively impact\nthe LLM effectiveness. Finally, and even more surprisingly, random,\nnoisy documents are actually helpful in increasing the accuracy of\nthese systems when correctly positioned within a prompt. While\nwe have proposed heuristics to exploit these findings, further re-\nsearch is needed both to uncover the inner mechanisms behind\nthis behavior and to develop a new generation of information re-\ntrieval techniques that are specifically designed to interact with the\ngenerative component.\n\nACKNOWLEDGMENTS\n\nThis work is supported by the Spoke \u201cFutureHPC & BigData\u201d of\nthe ICSC \u2013 Centro Nazionale di Ricerca in High-Performance Com-\nputing, Big Data and Quantum Computing, the Spoke \u201cHuman-\ncentered AI\u201d of the M4C2 - Investimento 1.3, Partenariato Esteso\nPE00000013 - \"FAIR - Future Artificial Intelligence Research\", SER-\nICS (PE00000014), IR0000013 - SoBigData.it, funded by European\nUnion \u2013 NextGenerationEU, the FoReLab project (Departments of\nExcellence), and the NEREO PRIN project funded by the Italian Min-\nistry of Education and Research Grant no. 2022AEFHAZ. This work\nwas carried out while Florin Cuconasu was enrolled in the Italian\nNational Doctorate on Artificial Intelligence run by the Sapienza\nUniversity of Rome.\n\n727\n\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\nFlorin Cuconasu et al.\n\nREFERENCES\n\n[1] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,\nRuxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien\nLaunay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier,\nand Guilherme Penedo. 2023. The Falcon Series of Open Language Models.\narXiv:2311.16867 [cs.CL]\n[2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.\nSelf-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.\narXiv:2310.11511 [cs.CL]\n[3] Giuseppe Attanasio, Debora Nozza, Dirk Hovy, and Elena Baralis. 2022. Entropy-\nbased attention regularization frees unintended bias mitigation from lists.\n[4] Andrea Bacciu, Florin Cuconasu, Federico Siciliano, Fabrizio Silvestri, Nicola\nTonellotto, and Giovanni Trappolini. 2023. RRAML: Reinforced Retrieval Aug-\nmented Machine Learning. In Proceedings of the Discussion Papers - 22nd Interna-\ntional Conference of the Italian Association for Artificial Intelligence (AIxIA 2023 DP)\nco-located with 22nd International Conference of the Italian Association for Artificial\nIntelligence (AIxIA 2023), Rome, Italy, November 6-9, 2023 (CEUR Workshop Proceed-\nings, Vol. 3537), Roberto Basili, Domenico Lembo, Carla Limongelli, and Andrea\nOrlandini (Eds.). CEUR-WS.org, 29\u201337. https://ceur-ws.org/Vol-3537/paper4.pdf\n[5] Andrea Bacciu, Giovanni Trappolini, Andrea Santilli, Emanuele Rodol\u00e0, and\nFabrizio Silvestri. 2023. Fauno: The Italian Large Language Model that will leave\nyou senza parole!. In Proceedings of the 13th Italian Information Retrieval Workshop\n(IIR 2023), Pisa, Italy, June 8-9, 2023 (CEUR Workshop Proceedings, Vol. 3448),\nFranco Maria Nardini, Nicola Tonellotto, Guglielmo Faggioli, and Antonio Ferrara\n(Eds.). CEUR-WS.org, 9\u201317. https://ceur-ws.org/Vol-3448/paper-24.pdf\n[6] Edward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lam-\nbert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023.\nOpen LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_\nllm_leaderboard.\n[7] Parishad BehnamGhader, Santiago Miret, and Siva Reddy. 2023. Can Retriever-\nAugmented Language Models Reason? The Blame Game Between the Retriever\nand the Language Model. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.).\nAssociation for Computational Linguistics, Singapore, 15492\u201315509.\nhttps:\n//doi.org/10.18653/v1/2023.findings-emnlp.1036\n[8] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-\nford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-\ndan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving\nfrom trillions of tokens. In International conference on machine learning. PMLR,\nBaltimora, 2206\u20132240.\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[10] Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and Effective Text Encoding\nfor Chinese LLaMA and Alpaca. arXiv preprint arXiv:2304.08177 (2023). https:\n//arxiv.org/abs/2304.08177\n[11] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,\nPierre-Emmanuel Mazar\u00e9, Maria Lomeli, Lucas Hosseini, and Herv\u00e9 J\u00e9gou. 2024.\nThe Faiss library. (2024). arXiv:2401.08281 [cs.LG]\n[12] Garrachonr. 2023. LlamaDos. https://github.com/Garrachonr/LlamaDos.\n[13] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.\nRetrieval augmented language model pre-training. In International conference on\nmachine learning. PMLR, Vienna, 3929\u20133938.\n[14] David T Hoffmann, Simon Schrodi, Nadine Behrmann, Volker Fischer, and\nThomas Brox. 2023. Eureka-Moments in Transformers: Multi-Step Tasks Reveal\nSoftmax Induced Optimization Problems.\n[15] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense infor-\nmation retrieval with contrastive learning.\n[16] Mojan Javaheripi, S\u00e9bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck,\nCaio C\u00e9sar Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan,\nSivakanth Gopi, et al. 2023. Phi-2: The surprising power of small language\nmodels.\n[17] jphme. 2023. Llama-2-13b-chat-german. https://huggingface.co/jphme/Llama-2-\n13b-chat-german.\n[18] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.\n2023. Large language models struggle to learn long-tail knowledge. In Proceedings\nof the 40th International Conference on Machine Learning (ICML\u201923). JMLR.org,\nHonolulu, Hawaii, USA, Article 641, 12 pages.\n[19] Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\n[20] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn,\n\nYulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,\n6769\u20136781. https://doi.org/10.18653/v1/2020.emnlp-main.550\n[21] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael\nBendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs.\narXiv preprint arXiv:2401.06954 (2024).\n[22] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of naacL-HLT, Vol. 1. Association for Computational Linguistic, Min-\nneapolis, 2.\n[23] Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby,\nfuzzy far away: How neural language models use context.\n[24] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage\nsearch via contextualized late interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research and development in Information\nRetrieval. ACM, Xi\u2019an, 39\u201348.\n[25] Bevan Koopman and Guido Zuccon. 2023. Dr ChatGPT tell me what I want to\nhear: How different prompts impact health answer correctness. In Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing,\nHouda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational\nLinguistics, Singapore, 15012\u201315022. https://doi.org/10.18653/v1/2023.emnlp-\nmain.928\n[26] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,\nKristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.\nDai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A\nBenchmark for Question Answering Research. Transactions of the Association for\nComputational Linguistics 7 (2019), 452\u2013466. https://doi.org/10.1162/tacl_a_00276\n[27] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent Retrieval for\nWeakly Supervised Open Domain Question Answering. In Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R.\nTraum, and Llu\u00eds M\u00e0rquez (Eds.). Association for Computational Linguistics,\nFlorence, 6086\u20136096. https://doi.org/10.18653/V1/P19-1612\n[28] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459\u20139474.\n[29] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar,\nand Yin Tat Lee. 2023. Textbooks are all you need ii: phi-1.5 technical report.\n[30] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\nFabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models\nuse long contexts.\n[31] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022.\nFantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot\nPrompt Order Sensitivity. In Proceedings of the 60th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan,\nPreslav Nakov, and Aline Villavicencio (Eds.). Association for Computational\nLinguistics, Dublin, Ireland, 8086\u20138098. https://doi.org/10.18653/v1/2022.acl-\nlong.556\n[32] C Manning, P Raghavan, and H Schutze. 2008. Term weighting, and the vector\nspace model. Cambridge University Press Cambridge, Cambridge. 109\u2013133 pages.\n[33] Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram\nPasunuru, Roberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language models: a survey.\n[34] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020.\nAmbigQA: Answering Ambiguous Open-domain Questions. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),\nBonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for\nComputational Linguistics, Online, 5783\u20135797. https://doi.org/10.18653/v1/2020.\nemnlp-main.466\n[35] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming\nCurated Corpora with Web Data, and Web Data Only. arXiv:2306.01116 [cs.CL]\n[36] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention\nwith Linear Biases Enables Input Length Extrapolation. https://openreview.net/\nforum?id=R8sQPpGCv0\n[37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.\nImproving language understanding by generative pre-training.\n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[39] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin\nLeyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language\nmodels.\n[40] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends\u00ae in Information Retrieval\n3, 4 (2009), 333\u2013389.\n\n728\n\nThe Power of Noise: Redefining Retrieval for RAG Systems\nSIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA\n\n[41] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan\nLiu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston.\n2021. Recipes for Building an Open-Domain Chatbot. In Proceedings of the\n16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.).\nAssociation for Computational Linguistics, Online, 300\u2013325. https://doi.org/10.\n18653/v1/2021.eacl-main.24\n[42] Gerard Salton and Michael J. McGill. 1983. Introduction to modern information\nretrieval. McGraw-Hill (1983).\n[43] Andrea Santilli and Emanuele Rodol\u00e0. 2023. Camoscio: an Italian Instruction-\ntuned LLaMA. arXiv:2307.16456 [cs.CL]\n[44] Artsiom Sauchuk, James Thorne, Alon Halevy, Nicola Tonellotto, and Fabrizio\nSilvestri. 2022. On the Role of Relevance in Natural Language Processing Tasks.\nIn Proceedings of the 45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval. ACM, Madrid, 1785\u20131789.\n[45] Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You\nNeed. arXiv:1911.02150 [cs.NE]\n[46] Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021.\nDo long-range language models actually use long-range context?\n[47] MosaicML NLP Team et al. 2023. Introducing mpt-7b: A new standard for open-\nsource, ly usable llms.\n[48] Gabriele Tolomei, Cesare Campagnano, Fabrizio Silvestri, and Giovanni Trap-\npolini. 2023. Prompt-to-OS (P2OS): Revolutionizing Operating Systems and\nHuman-Computer Interaction with Integrated AI Generative Models. In 5th\nIEEE International Conference on Cognitive Machine Intelligence, CogMI 2023, At-\nlanta, GA, USA, November 1-4, 2023. IEEE, 128\u2013134.\nhttps://doi.org/10.1109/\nCOGMI58952.2023.00027\n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models.\n[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\n[51] Giovanni Trappolini, Andrea Santilli, Emanuele Rodol\u00e0, Alon Y. Halevy, and\nFabrizio Silvestri. 2023. Multimodal Neural Databases. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, Hsin-Hsi Chen, Wei-Jou (Ed-\nward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete\n(Eds.). ACM, 2619\u20132628. https://doi.org/10.1145/3539618.3591930\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\n\nyou Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc., Long Beach.\nhttps://proceedings.neurips.cc/\npaper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[53] Michael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. 2017. TL;DR:\nMining Reddit to Learn Automatic Summarization. In Proceedings of the Workshop\non New Frontiers in Summarization, Lu Wang, Jackie Chi Kit Cheung, Giuseppe\nCarenini, and Fei Liu (Eds.). Association for Computational Linguistics, Copen-\nhagen, Denmark, 59\u201363. https://doi.org/10.18653/v1/W17-4508\n[54] Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, and Dacheng Tao. 2024.\nOOP: Object-Oriented Programming Evaluation Benchmark for Large Language\nModels. arXiv preprint arXiv:2401.06628 (2024).\n[55] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023.\nAdaptive\nchameleon or stubborn sloth: Revealing the behavior of large language mod-\nels in knowledge conflicts. In The Twelfth International Conference on Learning\nRepresentations.\n[56] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng\nCao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng\nXu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu.\n2024. OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in\nReal Computer Environments. arXiv:2404.07972 [cs.AI]\n[57] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers\nfor Text Ranking: BERT and Beyond. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies: Tutorials, Greg Kondrak, Kalina Bontcheva, and Dan\nGillick (Eds.). Association for Computational Linguistics, Online, 1\u20134.\nhttps:\n//doi.org/10.18653/v1/2021.naacl-tutorials.1\n[58] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Rama-\npuram, Yizhe Zhang, Jiatao Gu, and Joshua M Susskind. 2023. Stabilizing trans-\nformer training by preventing attention entropy collapse. In International Con-\nference on Machine Learning. PMLR, PMLR, Hawaii, 40770\u201340803.\n[59] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping\nMa. 2021. Optimizing Dense Retrieval Model Training with Hard Negatives.\nIn Proceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR \u201921). New York, NY, USA, 1503\u20131512.\n[60] Guido Zuccon, Bevan Koopman, and Razia Shaik. 2023. ChatGPT Hallucinates\nwhen Attributing Answers. In Proceedings of the Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval in the Asia Pacific\nRegion (Beijing, China) (SIGIR-AP \u201923). Association for Computing Machinery,\nNew York, NY, USA, 46\u201351. https://doi.org/10.1145/3624918.3625329\n\n729",
  "figures": [
    {
      "path": "data/processed/figures/The Power of Noise_ Redefining Retrieval for RAG Systems_p0_fig0.png",
      "page": 0,
      "caption": "",
      "ocr_text": ""
    },
    {
      "path": "data/processed/figures/The Power of Noise_ Redefining Retrieval for RAG Systems_p0_fig1.png",
      "page": 0,
      "caption": "",
      "ocr_text": "L)\n\nCheck for\nupdates"
    },
    {
      "path": "data/processed/figures/The Power of Noise_ Redefining Retrieval for RAG Systems_p6_fig0.png",
      "page": 6,
      "caption": "",
      "ocr_text": "Attention Layers\n\n\"\no\n\n-\nN\n\nN\nN\n\n25\n\nN\nB\n\n\u00a5\n\nAttention from Answer to Documents\n\n00\u00a2\u00ae goc > goe? ot ? poct poe? goe goe ot \u00ae o ? o 30\nDocuments in Context\n\no0e X o\n\n035\n\n030\n\n025\n\n0.20\n\n015\n\n-0.10\n\n-0.05"
    }
  ],
  "paper_id": "The Power of Noise_ Redefining Retrieval for RAG Systems",
  "metadata": {
    "title": "",
    "authors": "",
    "pages": 11
  }
}